\chapter{Einleitung}


Die Anfrageoptimierung ist ein Feld in der Datenbankforschung, die fast genau so lange existiert, wie das Feld selbst. Ein Teilproblem der Anfrageoptimierung ist die Bestimmung der optimalen Join-Reihenfolge. Also der Reihenfolge an Joins, mit der Joins am effektivsten abgearbeitet werden können.

Zur Bestimmung der Join-Reihenfolge werden Enumeratoren eingesetzt. Basierend auf einer initialen Anfrage, werden alternative Join-Reihenfolgen (Pläne) gebildet und der optimale Plan ausgewählt. Es lässt sich zwischen unterschiedlichen Arten der Join Enumeration unterscheiden: Auf der einen Seite die Top-Down Join-Enumeration. Sie bietet den Vorteil, dass bereits während der Ausführung die alternativen Pläne beschnitten werden können (Cost Based Pruning). Auf der anderen Seite kann bei der Bottom-Up Enumeration mit Hilfe von dynamischer Programmierung der beste Plan bestimmt werden. Neben diesen beiden Ansätzen besteht auch der regelbasierte Transformationsansatz, der beispielsweise in den Datenbanksystemen Volcano und Cascades zum Einsatz kommt.

Ziel aller Enumerationsverfahren ist es, basierend auf alternativen, kreuzproduktfreien Plänen, den optimalen Plan zu finden. Alle Pläne müssen krezuzproduktfrei sein, da angenommen werden kann, dass kreuzproduktfreie Pläne immer kosteneffizienter sind als Pläne, die Kreuzprodukte enthalten. Um einen Vergleich zwischen den Plänen zu ermöglichen, müssen immer alle Pläne, die kreuzproduktfrei sind, verglichen werden.




In dieser Arbeit wird der Ansatz der regelbasierten Transformation genauer unter die Lupe genommen. Basierend auf der Arbeit von \cite{shanbhag2014optimizing} werden Mengen von Transformationsregeln untersucht. Es wird das Ergebnis von \cite{shanbhag2014optimizing} geprüft und bestätigt, dass eine Regelmenge (RS-B2), die von \cite{pellenkoft1997complexity} vorgestellt wurden, nicht vollständig alle, alternativen Pläne erzeugen kann. In Experimenten wird die Vollständigkeit geprüft, indem die Ergebnisse mit anderen Regelmengen verglichen werden. Neben diesem Vergleich, wird auch die neue Regel RS-Graph untersucht. Die Regel RS-Graph ist eine neue Regelmenge, die aus der Regel GraphRule besteht. Sie implementiert Top-Down Enumerator zur Erstellung von alternativen Plänen. Dieser Enumerator wird auf seine Performance hin gepürft und mit den bestehenden Regelmengen verglichen. Im Gegensatz zu den Test, die durch \cite{shanbhag2014optimizing} in Java durchgeführt wurden und deren Messergebnisse Schwankungen unterlagen, wird in dieser Arbeit die Performancemessung mit einem speziell in C++ entwickelten Prototypen gemessen. Die Messergebnisse schwanken dank einer Implementierung in C++ erheblich weniger. Die Messergebnisse bzgl. der Performance der neuen Regelmenge werden nachvollzogen und bestätigt.




\section{Motivation}


Die Anfrageoptimierung ist einer der wichtigsten Bestandteile bei der Ausführung von Datenbankabfragen. Insbesondere im Kontext von komplexen und automatisch generierten SQL-Statements verspricht die Optimierung hohe Geschwindigkeitsverbesserungen. Der Optimierer ist für das Finden der richtigen Zugriffsstrategie auf Datensätze verantwortlich. Er entscheidet u.a. welche Art von Joins verwendet werden, in welcher Reihenfolge diese Joins angewendet werden, oder ob Index-Scans anstatt von Full-Table-Scans zum Einsatz kommen. Nur durch die erreichte Performance heute gängiger Datenbanksysteme sind Anwendungen und Informationssysteme wie \ac{ERP}, \ac{CRM}, \ac{CMS}, \ac{BDWH} möglich geworden. Die Datenbanksysteme sind ein integraler Bestandteil unserer heutigen Wissens- und Informationsgesellschaft. Der schnelle und effiziente Zugriff auf Informationen und damit die Nutzung von \ac{DBMS} ist aus unserer Welt nicht mehr wegzudenken. Anfrageoptimierung ist und bleibt daher ein wichtiger Teil.

Anfrageoptimierung ist ein seit Jahrzehnten bedeutungsvolles Thema für Datenbankhersteller. Erkenntnisse, die bereits in den 1970er Jahren gewonnen wurden, kommen auch in modernen Systemen zum Einsatz. Beispielsweise ist IBMs System R, das in den 1970er Jahren entwickelt wurde, die Grundlage für das heutige DB 2  \cite{wade2012ibm}. Auch Microsofts SQL Server lässt sich auf die Entwicklung von Volcano und EXODUS in den 1980er Jahren zurückführen.
In der langen Tradition der Anfrageoptimierer wurden die bekannten Lösungsansätze immer wieder kritisch betrachtet, weiterentwickelt und verbessert. 2014 wurden Regelsets von \cite{pellenkoft1997complexity}, die im Volcano Projekt zur Anpassung der Joinreihenfolge verwendet werden, von \cite{shanbhag2014optimizing} genauer unter die Lupe genommen.
Sie stellten fest, dass eine der verwendeten Regelmengen nicht dem Anspruch genügt, alle äquivalenten Pläne für eine gegebene Anfrage zu finden und somit die Gefahr besteht, dass ein suboptimaler Plan für die Ausführung gewählt werde. Neben der Unvollständigkeit einer Regelmenge wurde von Shanbhag et al. eine neue Transformationsregel \texttt{RS-Graph} vorgestellt. Sie soll die Geschwindigkeit moderner Join-Enumeratoren mit der Erweiterbarkeit von einem modularen System wie Volcano vereinen.

Diese Erkenntnisse bilden die Grundlage dieser Arbeit und werden im Folgenden eingeordnet, nachvollzogen und geprüft.

\section{Ziel der Arbeit}
Ziel der Arbeit ist es zunächst die Arbeit von \cite{shanbhag2014optimizing} in einen weiteren Kontext einzuordnen. Basierend auf der Einordnung und dem gewonnenen Wissen lassen sich die Fragen beantworten:

\begin{itemize}
\item Unter welchen Umständen ist die bemängelte Regelmenge unvollständig? 
\item Lassen sich die Ergebnisse von \cite{shanbhag2014optimizing} bzgl. ihres neuen Algorithmus und dessen Performanceversprechen auch in einer neuen Implementierung nachvollziehen?
\end{itemize}

Um erkennen zu können, ob bestehende Regelmengen unvollständig sind und als Grundlage für die neue Regelmenge dienen,  musste ein eigener Prototyp für die Erzeugung von äquivalenten Plänen implementiert werden. Basierend auf dem Prototypen konnten die Fragen untersucht und Ergebnisse gesammelt werden. Die gesammelten Erkenntnisse werden dann ausgewertet und interpretiert. 







\section{Inhaltlicher Aufbau}

Die Arbeit ist in fünf Teile gegliedert. Nach diesem ersten Kapitel, der Einleitung, wird Related Work besprochen, um die notwendigen Grundlagen für die spätere Arbeit zu liefern. Daraufhin folgt das dritte Kapitel Regeln und Regelmengen, das die Vorarbeit zu dieser Arbeit abrundet. Die Implementierungsarbeit wird im darauf folgenden Kapitel behandelt. Gewonnene Ergebnisse sind Teil von Kapitel 5. Die Arbeit wird mit einer Zusammenfassung in Kapitel 6 abgeschlossen.

Das Kapitel Related Work legt die Grundlagen und bietet einen Überblick über bereits geleistete Forschung. Zuerst wird die Architektur eines Datenbanksystems erläutert. Diese Architektur findet sich nicht nur in bekannten Forschungs- und kommerziellen Datenbanksystemen wieder, sondern ist auch Grundlage des entwickelten Prototypen. Neben diesen grundlegenden Strukturen werden auch die Begriffe Search Space, Enumerator und Kostenschätzung in Bezug gesetzt. Andere Datenbanksysteme wie System R, Grundlage für viele spätere Datenbanken, Starburst, Volcano mit seinem Vorgänger EXODUS und dem Nachfolger Cascades werden als Beispiele für Forschungssysteme aufgeführt. Auch das kommerziell erfolgreiche Datenbanksystem von Oracle wird in seinen Grundzügen erläutert. Neben diesem bekannten System wird auch Pyro(J), das von \cite{roy2001multi} entwickelt wurde, besprochen. Mit diesen Grundbegriffen, Grundlagen und Datenbanksystmen, werden im nächsten Kapitel Regelmengen und Regeln, die zur Optimierung einer Anfrage benötigt werden, behandelt.

In Kapitel drei werden zuerst die Regelmengen von \cite{pellenkoft1997complexity} vorgestellt. Sie kommen in Volcano, dieser ist bekannt aus Kapitel 2, zum Einsatz und wurden auch für Pyro(J) implementiert. Nachdem die Regelmengen \texttt{RS-B0}, \texttt{RS-B1} und \texttt{RS-B2} besprochen wurden, wird die Prüfung der Vollständigkeit der Regelmengen nachvollzogen. Auch die Unvollständigkeit von \texttt{RS-B2} wird angesprochen. Nachdem diese Prüfung besprochen wurde, kann die neue Regel GraphRule eingeführt werden und ihre Implementierung besprochen werden. Die konkret gemessenen Performanceverbesserungen schließen das Kapitel Regel und Regelmengen.



Um die Tests von \cite{shanbhag2014optimizing} nachzuvollziehen, wurde ein eigener Prototyp implementiert. Mit diesem Prototypen konnten die Ergebnisse nachvollzogen werden. Die Implementierung wird in Kapitel 4 besprochen. Dieses Kapitel startet mit einer Vorstellung der SOLID Prinzipien, durch deren Anwendung die Erweiterbarkeit und Wartbarkeit des Prototypen erzielt wurde. Nach deren Vorstellung wird eine Übersicht über den Optimierungsvorgang gegeben. Dieser Teil orientiert sich an dem Wissen, dass in Kapitel 2 gesammelt wurde. Im nächsten Schritt wird auf die einzelnen Komponenten des Prototypen eingegangen. Service Klassen, wie Logging, Zeitmesung und Operationen werden besprochen. Die Konfiguration, die aus einer HTML und einer C++ Komponente besteht, wird vorgestellt. Die notwendigen Datenstrukturen zur Speicherung von Anfragebäumen und deren Expansion werden ebenso vorgestellt, wie die Regeln und Regelmengen, die hierzu benötigt werden. Enumeratoren, die die Regelmengen ausführen, werden erläutert. Die Kostenschätzung, die ebenfalls einen wichtigen Teil der Untersuchung ausmacht, wird behandelt. Externe Adapter und andere Bestandteile sind auch Teil des Implementierungskapitels.


Nachdem die Implementierung abgeschlossen ist, kann die Evaluation beginnen. Die beiden Fragen nach Vollständigkeit der Regelmengen und der Performance von \texttt{RS-Graph} können mit praktischen Tests beantwortet werden. Doch zuvor muss geprüft werden, ob und inwiefern PyroJ geeignet war und ist, um diese Fragen zu beantworten. Hierzu wird zuerst die Wahl  der Plattform diskutiert und die konkrete Implementierung geprüft. Besonders die Wahl der Software Plattform Java für Performance-Tests wird in Frage gestellt. In der Evaluation werden Hypothesen über die Vollständigkeit und Geschwindigkeit der Regeln gestellt und beantwortet. Die Beantwortung der Fragen beginnt mit der Frage nach Vollständigkeit. Zuerst wird ein morphologischer Kasten gebildet, um eine Möglichkeit zur Übersicht über die potenziellen Testszenarien zu geben. Die Kombination der einzelnen Attribute des Kastens erlauben es, Tests schnell und einfach zu generieren. Es werden Hypothesen aufgestellt, die sich aus der Frage nach Vollständigkeit in Kombination mit den von \cite{shanbhag2014optimizing} erzielten Resultaten  beantworten. Zuerst werden Konstellationen gefunden in denen alle Regelmengen die gleichen (mutmaßlich) vollständigen Ergebnisse zurückgeben. Diese Szenarien werden so lange erweitert, bis unvollständige Fälle gefunden sind, die dann genauer geprüft und untersucht werden. So stellt sich heraus, dass die Regelmengen in azyklischen Join-Graphen vollständig sind.Diese Ergebnisse werden elaboriert und zusammengefasst. In einem dritten Teil wird die Performance der Regelmengen genau unter die Lupe genommen. Konkrete Tests, die sich aus der Arbeit von \cite{shanbhag2014optimizing} ableiten, prüfen dessen Ergebnisse. Trotz der Kritik, die im ersten Teil des Kapitels an dem Versuchsaufbau geäußert wurde, können die Ergebnisse nachvollzogen und präzisiert werden.

Geschlossen wird die Arbeit mit einem Rückblick auf die vorhergehenden Kapitel. Die Motivation wird wieder aufgenommen und reflektiert. Hierbei werden die Themen Related Work und Regeln und Reglmengen tangiert. Die Frage nach der Vollständigkeit der Regelmengen wird beantwortet und es wird festgestellt, dass die Regelmenge RS-B2 tatsächlich unvollständig ist, jedoch nur eingeschränkt. Die Messung der Performance der neuen Regelmenge \texttt{RS-Graph} wird beantwortet mit der Erkenntnis, dass Regeln, die auf Algorithmen die MinCutConservative basieren, tatsächlich performanter sind als traditionelle Regelmengen wie die von \cite{pellenkoft1997complexity}. Die Technik und die Implementierung, die diese Messungen erst möglich machen, wurde einbezogen. Bei der Implementierung dieser Arbeit und der Ausarbeitung ergaben sich Limitationen und Grenzen, die nicht überschritten werden konnten und die die Chance bieten basierend auf dieser Arbeit weiter zu forschen. Mit dem Nachziehen dieser Grenzen und einer Aufnahme der Einleitung schließt diese Arbeit.